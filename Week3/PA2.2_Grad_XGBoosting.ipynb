{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"PA 2.2: Gradient Boosting and XGBoost\"\n",
    "author: Daniel Chamberlin\n",
    "format:\n",
    "  html:\n",
    "    theme: cerulean \n",
    "    toc: true\n",
    "    embed-resources: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity Directions\n",
    "### Classifying Penguins\n",
    "\n",
    "Please review the following site for information on our dataset of interest here: https://allisonhorst.github.io/palmerpenguins (Links to an external site.)\n",
    "\n",
    "You can find the CSV file here: https://www.kaggle.com/datasets/parulpandey/palmer-archipelago-antarctica-penguin-data (Links to an external site.)\n",
    "\n",
    "This is a very nice, simple dataset with which to apply clustering techniques, classification techniques, or play around with different visualization methods. Your goal is to use the other variables in the measurement variables in the dataset to predict (classify) species.\n",
    "\n",
    "### Assignment Specs\n",
    "\n",
    "- You should compare XGBoost or Gradient Boosting to the results of your previous AdaBoost activity.\n",
    "- Based on the visualizations seen at the links above you're probably also thinking that this classification task should not be that difficult. So, a secondary goal of this assignment is to test the effects of the XGBoost (or Gradient Boosting) function arguments on the algorithm's performance. \n",
    "- You should explore at least 3 different sets of settings for the function inputs, and you should do your best to find values for these inputs that actually change the results of your modelling. That is, try not to run three different sets of inputs that result in the same performance. The goal here is for you to better understand how to set these input values yourself in the future. Comment on what you discover about these inputs and how the behave.\n",
    "- Your submission should be built and written with non-experts as the target audience. All of your code should still be included, but do your best to narrate your work in accessible ways.\n",
    "\n",
    "# Notes\n",
    "### Gradient Boosting\n",
    "- After the initial model is fit, a loss function is plotted (instead of updating the weights as we did in AdaBoost)\n",
    "- Gradient Boosting gets its name from Gradient Descent, which is the method used to find the parameters which minimize the loss function\n",
    "### XGBoost\n",
    "- Direct application of Gradient Boosting for decision trees with the following advantages:\n",
    "1. Easy to use\n",
    "2. Computational Efficiency\n",
    "3. Model Accuracy\n",
    "4. Feasibility â€“ easy to tune parameters and modify objectives\n",
    "\n",
    "# Process\n",
    "## import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>MALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>FEMALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>40.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>FEMALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>36.7</td>\n",
       "      <td>19.3</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>FEMALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.3</td>\n",
       "      <td>20.6</td>\n",
       "      <td>190.0</td>\n",
       "      <td>3650.0</td>\n",
       "      <td>MALE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
       "0  Adelie  Torgersen            39.1           18.7              181.0   \n",
       "1  Adelie  Torgersen            39.5           17.4              186.0   \n",
       "2  Adelie  Torgersen            40.3           18.0              195.0   \n",
       "4  Adelie  Torgersen            36.7           19.3              193.0   \n",
       "5  Adelie  Torgersen            39.3           20.6              190.0   \n",
       "\n",
       "   body_mass_g     sex  \n",
       "0       3750.0    MALE  \n",
       "1       3800.0  FEMALE  \n",
       "2       3250.0  FEMALE  \n",
       "4       3450.0  FEMALE  \n",
       "5       3650.0    MALE  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\n",
    "peng = pd.read_csv(url)\n",
    "\n",
    "peng = peng.dropna()\n",
    "peng.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Encode categorical variables\n",
    "peng_encoded = peng.copy()\n",
    "peng_encoded['species'] = LabelEncoder().fit_transform(peng['species'])\n",
    "peng_encoded['sex'] = LabelEncoder().fit_transform(peng['sex'])\n",
    "peng_encoded['island'] = LabelEncoder().fit_transform(peng['island'])\n",
    "\n",
    "# Define features and target\n",
    "X = peng_encoded.drop('species', axis=1)\n",
    "y = peng_encoded['species']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Results:\n",
      "    n_estimators  learning_rate  accuracy\n",
      "0              1            0.5  0.985075\n",
      "1              1            1.0  1.000000\n",
      "2              1            1.5  1.000000\n",
      "3             10            0.5  1.000000\n",
      "4             10            1.0  1.000000\n",
      "5             10            1.5  0.970149\n",
      "6             25            0.5  1.000000\n",
      "7             25            1.0  1.000000\n",
      "8             25            1.5  0.970149\n",
      "9            100            0.5  1.000000\n",
      "10           100            1.0  1.000000\n",
      "11           100            1.5  0.970149\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Gradient Boosting model\n",
    "params = [\n",
    "    {\"n_estimators\": 1, \"learning_rate\": 0.5},\n",
    "    {\"n_estimators\": 1, \"learning_rate\": 1.0},\n",
    "    {\"n_estimators\": 1, \"learning_rate\": 1.5},\n",
    "    {\"n_estimators\": 10, \"learning_rate\": 0.5},\n",
    "    {\"n_estimators\": 10, \"learning_rate\": 1.0},\n",
    "    {\"n_estimators\": 10, \"learning_rate\": 1.5},\n",
    "    {\"n_estimators\": 25, \"learning_rate\": 0.5},\n",
    "    {\"n_estimators\": 25, \"learning_rate\": 1.0},\n",
    "    {\"n_estimators\": 25, \"learning_rate\": 1.5},\n",
    "    {\"n_estimators\": 100, \"learning_rate\": 0.5},\n",
    "    {\"n_estimators\": 100, \"learning_rate\": 1.0},\n",
    "    {\"n_estimators\": 100, \"learning_rate\": 1.5}\n",
    "]\n",
    "\n",
    "gb_results = []\n",
    "for p in params:\n",
    "    model = GradientBoostingClassifier(**p)\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    gb_results.append((p[\"n_estimators\"], p[\"learning_rate\"], acc))\n",
    "\n",
    "# Show results\n",
    "gb_results_df = pd.DataFrame(gb_results, columns=[\"n_estimators\", \"learning_rate\", \"accuracy\"])\n",
    "print(\"Gradient Boosting Results:\")\n",
    "print(gb_results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Results:\n",
      "    n_estimators  learning_rate  accuracy\n",
      "0              1            0.5       1.0\n",
      "1              1            1.0       1.0\n",
      "2              1            1.5       1.0\n",
      "3             10            0.5       1.0\n",
      "4             10            1.0       1.0\n",
      "5             10            1.5       1.0\n",
      "6             25            0.5       1.0\n",
      "7             25            1.0       1.0\n",
      "8             25            1.5       1.0\n",
      "9            100            0.5       1.0\n",
      "10           100            1.0       1.0\n",
      "11           100            1.5       1.0\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# XGBoost model\n",
    "params = [\n",
    "    {\"n_estimators\": 1, \"learning_rate\": 0.5},\n",
    "    {\"n_estimators\": 1, \"learning_rate\": 1.0},\n",
    "    {\"n_estimators\": 1, \"learning_rate\": 1.5},\n",
    "    {\"n_estimators\": 10, \"learning_rate\": 0.5},\n",
    "    {\"n_estimators\": 10, \"learning_rate\": 1.0},\n",
    "    {\"n_estimators\": 10, \"learning_rate\": 1.5},\n",
    "    {\"n_estimators\": 25, \"learning_rate\": 0.5},\n",
    "    {\"n_estimators\": 25, \"learning_rate\": 1.0},\n",
    "    {\"n_estimators\": 25, \"learning_rate\": 1.5},\n",
    "    {\"n_estimators\": 100, \"learning_rate\": 0.5},\n",
    "    {\"n_estimators\": 100, \"learning_rate\": 1.0},\n",
    "    {\"n_estimators\": 100, \"learning_rate\": 1.5}\n",
    "]\n",
    "\n",
    "xgb_results = []\n",
    "for p in params:\n",
    "    model = XGBClassifier(eval_metric='mlogloss', **p)\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    xgb_results.append((p[\"n_estimators\"], p[\"learning_rate\"], acc))\n",
    "\n",
    "# Show results\n",
    "xgb_results_df = pd.DataFrame(xgb_results, columns=[\"n_estimators\", \"learning_rate\", \"accuracy\"])\n",
    "print(\"XGBoost Results:\")\n",
    "print(xgb_results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that for both of these methods, on the penguins dataset, we are getting very high accuracy scores. This is similar to what we saw with the AdaBoosting method as well and is likely a result of an easily classified dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
